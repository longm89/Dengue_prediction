---
title: "Dengue Forecasting"
author: "Axel Forveille, MAI Tien Long"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r echo=FALSE, message=FALSE}
##### Start an R session and import the library
rm(list=objects())
```
```{r echo = FALSE, message = FALSE}
# import the library
library(ggplot2)
library(corrplot)
library(dplyr)
library(lubridate)
library(scales)
library(gridExtra)
library(ggthemes)
library(tidyquant)
library(forecast)
library(TSA) # for plotting periodogram
```
## Introduction 
The purpose of the project is to predict the number of dengue cases in two cities, San Juan and Iquitos for each week, using environmental and climate variables. The challenge was organized in 2015 by several departments in the U.S. Federal Government (Department of Health and Human Services, Department of Defense, Department of Commerce, and the Department of Homeland Security), with the support of the Pandemic Prediction and Forecasting Science and Technology Interagency Working Group under the National Science and Technology Council (https://dengueforecasting.noaa.gov/).

#### Data 
The data for each city consists of:   

* Time indicators
* NOAA's GHCN daily climate data weather station measurements.     
* PERSIANN satellite precipitation measurements.      
* NOAA's NCEP Climate Forecast System Reanalysis measurements.
* Satellite vegetation. 
* The number of cases for each week.

Additionally, we downloaded the environmental, social and economic data from WorldBank and we chose several parameters that might explain the number of cases:

* forest_area_sq_km 
* Total population       
* population_density_people_per_sq_km_of_land_area       
* gdp_current_us        
* employment_to_population_average      
* Population age percentage: 0 - 9, 10 - 20, 20 - 60, 60+            

Another data that can affect the spread of the disease is the number of migration, however, we couldn't find the data.

#### Methods

##### Linear Regression

##### Generalized Linear Models with the Negative Binomial Distribution family

##### Model GAM 

##### Regression Tree 

##### Random Forest

##### Times Series (ARIMA and SARIMA)

##### Gradient Boosting

##### Expert Aggregation

## Data Wrangling and Exploration 
#### Data Wrangling   
##### Data from the challenge 
The data for each city consists of:   

* Time indicators:     
  + week_start_date  
  + weekofyear
  + year
* NOAA's GHCN daily climate data weather station measurements.     
  + station_max_temp_c - maximum temperature          
  + station_min_temp_c - minimum temperature          
  + station_avg_temp_c - average temperature          
  + station_precip_mm - total precipitation           
  + station_diur_temp_rng_c - diurnal temperature range
* PERSIANN satellite precipitation measurements.      
  + precipitation_amt_mm - total precipitation       
* NOAA's NCEP Climate Forecast System Reanalysis measurements.
  + reanalysis_sat_precip_amt_mm – Total precipitation
  + reanalysis_dew_point_temp_k – Mean dew point temperature
  + reanalysis_air_temp_k – Mean air temperature
  + reanalysis_relative_humidity_percent – Mean relative humidity
  + reanalysis_specific_humidity_g_per_kg – Mean specific humidity
  + reanalysis_precip_amt_kg_per_m2 – Total precipitation
  + reanalysis_max_air_temp_k – Maximum air temperature
  + reanalysis_min_air_temp_k – Minimum air temperature
  + reanalysis_avg_temp_k – Average air temperature
  + reanalysis_tdtr_k – Diurnal temperature range
* Satellite vegetation. 
  + ndvi_se – Pixel southeast of city centroid
  + ndvi_sw – Pixel southwest of city centroid
  + ndvi_ne – Pixel northeast of city centroid
  + ndvi_nw – Pixel northwest of city centroid
  
We separate the data into two parts for each country and add the missing values using spline interpolation.   

##### Data from WorldBank           

We download the data from WorldBank, clean and select important variables that might contribute to the prediction of the number of Dengue cases:
* Total population       
* population_density_people_per_sq_km_of_land_area       
* forest_area_sq_km       
* gdp_current_us        
* employment_to_population_average      
* Population age percentage: 0 - 9, 10 - 20, 20 - 60, 60+

We see that all the explicable variables make sense. We can group them into the following groups:       
* Time of the year 
* Climate variables       
* Total population       
* Population density       
* Population age      
* Economical condition   
There are several variables from climate variables correlate with each other. 

```{r}

# import the cleaned data
load("rdas/merged_iq_train.rda")
load("rdas/merged_sj_train.rda")
```
##### Features engineering        
In later sections, we will apply classical machine learning models such that Generalized Linear Model and Generalized Additive Model. As the number of cases is a time series: the number of cases of a week is highly influenced by the number of cases of the previous week, in order to have independent residual errors, we will add lag variables to reduce their auto-correlation. Our model will be able to use the number of cases from the past weeks to predict the number of cases of the current week. 
```{r}
#For Iquitos, we add the lagging variables by 1 week, 3 weeks, 5 weeks
merged_iq_train <- merged_iq_train %>%
  mutate(lag_1_total_cases = lag(total_cases))
merged_iq_train[1, "lag_1_total_cases"] = 0
merged_iq_train <- merged_iq_train %>%
  mutate(lag_3_total_cases = lag(lag(lag_1_total_cases)))
merged_iq_train[1:3, "lag_3_total_cases"] = 0
merged_iq_train <- merged_iq_train %>%
  mutate(lag_4_total_cases = lag(lag_3_total_cases))
merged_iq_train[1:4, "lag_4_total_cases"] = 0
merged_iq_train <- merged_iq_train %>%
  mutate(lag_5_total_cases = lag(lag(lag_3_total_cases)))
merged_iq_train[1:5, "lag_5_total_cases"] = 0
```
We add a dummy variable Time term to model trend in the data over time. 
```{r}
# We add the Time term 
merged_iq_train$Time <- seq.int(nrow(merged_iq_train))
```


#### Data Exploration   


##### Visualising the number of cases over the year:
```{r echo = FALSE, message = FALSE}
######## For Iquitos
# Define Start and end times for the subset as R objects that are the time class
startTime_iq <- as.Date("2001-01-01")
endTime_iq <- as.Date("2009-12-24")
# create a start and end time R object
limits_iq <- c(startTime_iq, endTime_iq)
iq_weekly_cases <- ggplot(merged_iq_train, aes(week_start_date, total_cases)) +
  geom_line(na.rm=TRUE) + 
  geom_ma(ma_fun = SMA, n = 52) + # moving average with period of 52 weeks to detect trend
  ggtitle("Total number of cases from 2001 - 2010 in Iquitos") +
  xlab("Date") + ylab("Total number of cases")
# format x-axis: dates
iq_weekly_cases_time_series <- iq_weekly_cases + 
  (scale_x_date(limits = limits_iq,
                breaks = date_breaks("1 year"),
                labels = date_format("%b %y")))
iq_weekly_cases_time_series

# looking for seasonality over the week of the year
merged_iq_train %>% filter("2001" <= year & year <= "2009") %>% 
  ggplot(aes(x = weekofyear, y = total_cases, color = factor(year))) +
  geom_line()

acf(merged_iq_train$total_cases, lag.max = 52 * 3)
```

```{r echo = FALSE, message = FALSE}
# For San Juan
# create a start and end time R object
startTime_sj <- as.Date("1990-04-30")
endTime_sj <- as.Date("2000-04-22")

limits_sj <- c(startTime_sj, endTime_sj)
sj_weekly_cases <- ggplot(merged_sj_train, aes(week_start_date, total_cases)) +
  geom_line(na.rm=TRUE) + 
  geom_ma(ma_fun = SMA, n = 52) + # moving average with period of 52 weeks to detect trend
  ggtitle("Total number of cases from 1990 - 2008 in San Juan") +
  xlab("Date") + ylab("Total number of cases")
# format x-axis: dates
sj_weekly_cases_time_series <- sj_weekly_cases + 
  (scale_x_date(limits = limits_sj,
                breaks = date_breaks("1 year"),
                labels = date_format("%b %y")))

sj_weekly_cases_time_series

# look for seasonality, depending on the week of the year
merged_sj_train %>% filter("1991" <= year & year <= "1999") %>% 
  ggplot(aes(x = weekofyear, y = total_cases, color = factor(year))) +
  geom_line()
acf(merged_sj_train$total_cases, lag.max = 52*3)
```
```{r}
# draw the two histograms
iq_histogram <- ggplot(data=merged_iq_train, aes(total_cases)) +
  geom_histogram(aes(y =..density..), fill = "orange") +
  geom_density()

sj_histogram <- ggplot(data=merged_sj_train, aes(total_cases)) +
  geom_histogram(aes(y =..density..), fill = "orange") +
  geom_density()

grid.arrange(iq_histogram, sj_histogram, ncol=2)
mean(merged_iq_train$total_cases)
var(merged_iq_train$total_cases)

mean(merged_sj_train$total_cases)
var(merged_sj_train$total_cases)
```

Some observations:

* There are more cases in San Juan than Iquitos   
* There are the peaks that we would like to predict. They happen in what months? Are there periods or seasonality trends?       
* The number of cases for both cities do not follow Gaussian distributions. It suggests that we should use Generalized Linear Model, as described in Simon N. Wood's book: Generalized Additive Models. The number of cases are natural numbers, we will assume that they follow Poisson distribution or Negative Binomial distribution, and in particular Negative Binomial distribution as the mean is much smaller than the variance. In fact, the Poisson distribution can be interpreted as a special case of the Negative Binomial distribution when the parameter $r \rightarrow \infty$ and is used as a way to model an over-dispersed Poisson distribution.



## Modeling and Prediction
### Preparing the data for training and testing
In the following, we will split the data into a training set and a testing set. As our data is time series, instead of a random selection of each set, we split our time series with respect to chronology, in order to train our models on the past data, and test the predictions on the future. 

```{r}
# For Iquitos
iq_train_size <- round(nrow(merged_iq_train) * 0.8)
iq_train <- merged_iq_train %>% slice(1: iq_train_size)
iq_test <- merged_iq_train %>% slice(iq_train_size + 1: nrow(merged_iq_train))


# For San Juan

sj_train_size <- round(nrow(merged_sj_train) * 0.8)
sj_train <- merged_sj_train %>% slice(1: sj_train_size)
sj_test <- merged_sj_train %>% slice(sj_train_size + 1: nrow(merged_sj_train))

```

We will use MAE score to evaluate different models.
```{r}
mae<-function(y, ychap)
{
  return(round(mean(abs(y-ychap)), digit = 2))
}
```

### Generalized Linear Models with Negative Binomial Distribution family.

### GAM models
```{r echo = FALSE, message = FALSE}
library(mgcv)
library(tidymv)
library(mgcViz)
```
In the following, we fit GAM models using temperature features, the population feature and moreover, Time, weekofyear to model trend and seasonality. After that, we look at another model, this time adding auto-regressive features such as lag variables.
For Iquitos: 
```{r}
g0 <- gam(total_cases ~ s(reanalysis_specific_humidity_g_per_kg) + s(reanalysis_dew_point_temp_k) + s(ndvi_sw) + s(reanalysis_air_temp_k)
  + s(population_total) 
  ,family = nb(), data = iq_train, method = "REML", select = TRUE)
#gam.check(g0)
summary(g0)
#plot.gam(g0)


# on train set
ychap <- predict(g0, newdata = iq_train, type = "response")
plot(iq_train$total_cases, type='l')
lines(ychap,col='red')

startTime_iq <- as.Date("2001-01-01")
endTime_iq <- as.Date("2009-12-24")
# create a start and end time R object
limits_iq <- c(startTime_iq, endTime_iq)
iq_weekly_cases <- ggplot(iq_train, aes(week_start_date, total_cases - ychap)) +
  geom_line(na.rm=TRUE) + 
  geom_ma(ma_fun = SMA, n = 52) + # moving average with period of 52 weeks to detect trend
  ggtitle("Residuals from 2001 - 2010 in Iquitos") +
  xlab("Date") + ylab("Total number of cases")
iq_weekly_cases
# on test set
ychap <- predict(g0, newdata = iq_test, type = "response")
mae(iq_test$total_cases, ychap)
plot(iq_test$total_cases, type='l')
lines(ychap,col='red')

```

```{r}
g1 <- gam(total_cases ~ s(reanalysis_specific_humidity_g_per_kg) + s(reanalysis_dew_point_temp_k) + s(ndvi_sw)
  + s(population_total) 
  + s(Time) + s(weekofyear, bs = 'cc', k = 52) 

  ,family = nb(), data = iq_train, method = "REML", select = TRUE)
#gam.check(g1)
summary(g1)
#plot.gam(g1)


# on train set
train_ychap <- predict(g1, newdata = iq_train, type = "response")
plot(iq_train$total_cases, type='l')
lines(train_ychap,col='red')
# look at ACF and PACF of residuals to see if there's auto-correlation

acf(iq_train$total_cases - train_ychap)
pacf(iq_train$total_cases - train_ychap)
startTime_iq <- as.Date("2001-01-01")
endTime_iq <- as.Date("2009-12-24")
# create a start and end time R object
limits_iq <- c(startTime_iq, endTime_iq)
iq_weekly_cases <- ggplot(iq_train, aes(week_start_date, total_cases - train_ychap)) +
  geom_line(na.rm=TRUE) + 
  geom_ma(ma_fun = SMA, n = 52) + # moving average with period of 52 weeks to detect trend
  ggtitle("Residuals from 2001 - 2010 in Iquitos") +
  xlab("Date") + ylab("Total number of cases")
iq_weekly_cases
# on test set
test_ychap <- predict(g1, newdata = iq_test, type = "response")
mae(iq_test$total_cases, test_ychap)
plot(iq_test$total_cases, type='l')
lines(test_ychap,col='red')

```

```{r}
g2 <- gam(total_cases ~ s(reanalysis_specific_humidity_g_per_kg) + s(reanalysis_dew_point_temp_k) + s(ndvi_sw)
  + s(population_total) 
  + s(Time) + s(weekofyear, bs = 'cc', k = 52) 
  + s(log(lag_1_total_cases+1)) + s(log(lag_3_total_cases+1))
  ,family = nb(), data = iq_train, method = "REML")
#gam.check(g2)
summary(g2)
#plot.gam(g2)


# on train set
train_ychap <- predict(g2, newdata = iq_train, type = "response")
plot(iq_train$total_cases, type='l')
lines(train_ychap, col='red')
# look at ACF and PACF of residuals to see if there's auto-correlation

acf(iq_train$total_cases - train_ychap)
pacf(iq_train$total_cases - train_ychap)

startTime_iq <- as.Date("2001-01-01")
endTime_iq <- as.Date("2009-12-24")
# create a start and end time R object
limits_iq <- c(startTime_iq, endTime_iq)
iq_weekly_cases <- ggplot(iq_train, aes(week_start_date, total_cases - train_ychap)) +
  geom_line(na.rm=TRUE) + 
  geom_ma(ma_fun = SMA, n = 52) + # moving average with period of 52 weeks to detect trend
  ggtitle("Residuals from 2001 - 2010 in Iquitos") +
  xlab("Date") + ylab("Total number of cases")
iq_weekly_cases
# on test set
test_ychap <- predict(g2, newdata = iq_test, type = "response")
mae(iq_test$total_cases, test_ychap)
plot(iq_test$total_cases, type='l')
lines(test_ychap,col='red')

```

### Regression Tree 
In this section, we look at Regression tree, a simple method that doesn't require any assumption on the distribution of the variables, and moreover, gives a way to select variables.
```{r message = FALSE}
library(rpart)
library(tree)
library(rpart.plot)
library(vip) # for feature importance
```
We first look at Iquitos:

```{r echo = TRUE, MESSAGE = FALSE}

tree_iq <- rpart(formula = total_cases ~ ., 
                 data = iq_train,
                 method = "anova")
rpart.plot(tree_iq)
ychap.tree_iq <- predict(tree_iq, newdata = iq_train)
plot(iq_train$total_cases, type='l')
lines(ychap.tree_iq,col='red')
ychap.tree_iq <- predict(tree_iq, newdata = iq_test)
mae(iq_test$total_cases, ychap.tree_iq)
plot(iq_test$total_cases, type='l')
lines(ychap.tree_iq,col='red')
print(vip(tree_iq, num_features = 10, bar = FALSE)) # return the importance of the features
```
For San Juan:      
```{r echo = TRUE, MESSAGE = FALSE, results='hide'}
# remove the results = 'hide' to see all the graphs for all the models in the Cross Validation.
tree_sj <- rpart(formula = total_cases ~ ., 
                 data = sj_train,
                 method = "anova")
rpart.plot(tree_sj)
ychap.tree_sj <- predict(tree_sj, newdata = sj_train)
plot(sj_train$total_cases, type='l')
lines(ychap.tree_sj,col='red')
ychap.tree_sj <- predict(tree_sj, newdata = sj_test)
mae(sj_test$total_cases, ychap.tree_sj)
plot(sj_test$total_cases, type='l')
lines(ychap.tree_sj,col='red')
print(vip(tree_sj, num_features = 10, bar = FALSE)) # return the importance of the features

```

#### Conclusion:
##### For Iquitos:       



##### For San Juan:     


### Random Forest

### Times Series (ARIMA and SARIMA model)
We first fit an ARIMA model and then a SARIMA model.
#### ARIMA model
### Gradient Boosting

### Expert Aggregation 

## Conclusion


