---
title: "Dengue Forecasting"
author: "Axel Forveille, MAI Tien Long"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r echo=FALSE, message=FALSE}
##### Start an R session and import the library
rm(list=objects())
```
## Introduction 
The purpose of the project is to predict the number of dengue cases in two cities, San Juan and Iquitos for each week, using environmental and climate variables. The challenge was organized in 2015 by several departments in the U.S. Federal Government (Department of Health and Human Services, Department of Defense, Department of Commerce, and the Department of Homeland Security), with the support of the Pandemic Prediction and Forecasting Science and Technology Interagency Working Group under the National Science and Technology Council (https://dengueforecasting.noaa.gov/).

#### Data 
The data for each city consists of:   

* week indicators.   
* NOAA's GHCN daily climate data weather station measurements.  
* PERSIANN satellite precipitation measurements.
* NOAA's NCEP Climate Forecast System Reanalysis measurements.
* Satellite vegetation. 
* Prediction for the number of cases for each week.

Additionally, we downloaded the social and economic data from WorldBank and we chose several parameters that might explain the number of cases:

* Total population       
* population_density_people_per_sq_km_of_land_area       
* forest_area_sq_km       
* gdp_current_us        
* employment_to_population_average      
* Population age percentage: 0 - 9, 10 - 20, 20 - 60, 60+            

Another data that can affect the spread of the disease is the number of migration, however, we couldn't find the data.

#### Methods

##### Negative Binomial Distribution model
As the total number of cases does not follow a Gaussian distribution, and this is a counting variable, we should use the Poisson distribution or the Negative Binomial distribution model. 

##### Model GAM 

##### Regression Tree 

##### Random Forest

##### Times Series

##### Gradient Boosting

## Data Wrangling and Exploration 
#### Data Wrangling   
##### Data from the challenge 

We separate the data into two parts for each country and add the missing values using spline interpolation.   

##### Data from WorldBank           

We download the data from WorldBank and we select important variables that might contribute to the prediction of the number of Dengue cases:
* Total population       
* population_density_people_per_sq_km_of_land_area       
* forest_area_sq_km       
* gdp_current_us        
* employment_to_population_average      
* Population age percentage: 0 - 9, 10 - 20, 20 - 60, 60+  

#### Data Exploration   
```{r echo = FALSE, message = FALSE}
# import the library
library(ggplot2)
library(corrplot)
library(dplyr)
library(lubridate)
library(scales)
library(gridExtra)
library(ggthemes)
```
For each city, we have the following variables:
```{r}

# import the cleaned data
load("rdas/merged_iq_train.rda")
load("rdas/merged_sj_train.rda")
names(merged_iq_train)
```

##### Visualising the number of cases over the year:
```{r echo = FALSE, message = FALSE}
######## Drawing the two time series
# for Iquitos
# Define Start and end times for the subset as R objects that are the time class
startTime_iq <- as.Date("2001-01-01")
endTime_iq <- as.Date("2010-01-01")
# create a start and end time R object
limits_iq <- c(startTime_iq, endTime_iq)
iq_weekly_cases <- ggplot(merged_iq_train, aes(week_start_date, total_cases)) +
  geom_line(na.rm=TRUE) + 
  ggtitle("Total number of cases from 2001 - 2010 in Iquitos") +
  xlab("Date") + ylab("Total number of cases")
# format x-axis: dates
iq_weekly_cases_time_series <- iq_weekly_cases + 
  (scale_x_date(limits = limits_iq,
                breaks = date_breaks("1 year"),
                labels = date_format("%b %y")))

iq_weekly_cases_time_series

# for San Juan
# create a start and end time R object
startTime_sj <- as.Date("1990-04-30")
endTime_sj <- as.Date("2000-04-22")

limits_sj <- c(startTime_sj, endTime_sj)
sj_weekly_cases <- ggplot(merged_sj_train, aes(week_start_date, total_cases)) +
  geom_line(na.rm=TRUE) + 
  ggtitle("Total number of cases from 1990 - 2008 in San Juan") +
  xlab("Date") + ylab("Total number of cases")
# format x-axis: dates
sj_weekly_cases_time_series <- sj_weekly_cases + 
  (scale_x_date(limits = limits_sj,
                breaks = date_breaks("1 year"),
                labels = date_format("%b %y")))

sj_weekly_cases_time_series


```
```{r}
# draw the two histograms
iq_histogram <- ggplot(data=merged_iq_train, aes(total_cases)) +
  geom_histogram(aes(y =..density..), fill = "orange") +
  geom_density()

sj_histogram <- ggplot(data=merged_sj_train, aes(total_cases)) +
  geom_histogram(aes(y =..density..), fill = "orange") +
  geom_density()

grid.arrange(iq_histogram, sj_histogram, ncol=2)
mean(merged_iq_train$total_cases)
var(merged_iq_train$total_cases)

mean(merged_sj_train$total_cases)
var(merged_sj_train$total_cases)
```

Some observations:

* There are more cases in San Juan than Iquitos   
* There are the peaks that we would like to predict. They happen in what months? Are there periods or seasonality trends?       
* The number of cases for both cities do not follow Gaussian distributions. It suggests that we should use Generalized Linear Model, as described in Simon N. Wood's book: Generalized Additive Models. The number of cases are natural numbers, we will assume that they follow Poisson distribution or Negative Binomial distribution, and in particular Negative Binomial distribution as the mean is much smaller than the variance. In fact, the Poisson distribution can be interpreted as a special case of the Negative Binomial distribution when the parameter $r \rightarrow \infty$ and is used as a way to model an over-dispersed Poisson distribution.

## Modeling and Prediction
### Preparing the data for training and testing
A simple method for training and evaluating the models, simple validation, is to split the data into training set and testing set. As our data is time series, instead of a random split, we should split our time series with respect to chronology, so that we could train our models on the past data, and test the predictions on the future. To improve consistency, we will use instead an analogue of cross validation for time series, called sliding windows. 
```{r fig.align = 'center', out.width = "50%", fig.cap = "The Sliding Windows method, the picture is taken from Uber's Engineering blog: https://eng.uber.com/omphalos/"}
knitr::include_graphics(here::here("", "figs/sliding_windows_method.png"))
```
To implement the method, we will choose the length of the training set train_size, the length of the test set test_size, and the step_size that we will move. The step_size corresponds to the length of the dropped part in Pass 2 in the picture. We implement the function below to build a list of (start_train, end_train, start_test, end_test) to train and evaluate the models.
```{r}
# The following function implements the Sliding windows method, 
# an analogue of Cross Validation for Time Series
# The following function receives an input of train_size, test_size, step_size 
# and returns a list of vectors of 4 parameters:
# (start_train, end_train, start_test, end_test)

build_sliding_windows <- function(dataset, train_size, test_size, step_size) {
  start_train <- 1 
  end_train <- start_train + train_size - 1
  start_test <- end_train + 1
  end_test <- start_test + test_size - 1
  
  plans <- list()
  while (end_test < nrow(dataset)) {
    plans_length = length(plans)
    plans[[plans_length + 1]] <- c(start_train, end_train, start_test, end_test)
    
    start_train <- start_train + step_size
    end_train <- start_train + train_size - 1
    start_test <- end_train + 1
    end_test <- start_test + test_size - 1
  }
  plans
}
```
Applying to our dataset, we will choose:          
* train_size = 60% of the length of the dataset       
* test_size = 20 % of the length of the dataset          
* step_size = 20 weeks      
The procedure then gives 10 and 6 folds for 
```{r}
sj_train_size <- round(nrow(merged_sj_train) * 0.6)
sj_test_size <- round(nrow(merged_sj_train) * 0.2)
sj_step_size <- 20
sj_plans <- build_sliding_windows(merged_sj_train, sj_train_size, sj_test_size, sj_step_size)
sj_plans

```
```{r}
iq_train_size <- round(nrow(merged_iq_train) * 0.6)
iq_test_size <- round(nrow(merged_iq_train) * 0.2)
iq_step_size <- 20
iq_plans <- build_sliding_windows(merged_iq_train, iq_train_size, iq_test_size, iq_step_size)
iq_plans

```
We will use this procedure to evaluate different models later.
```{r}
mae<-function(y, ychap)
{
  return(round(mean(abs(y-ychap)), digit = 2))
}
```

### Generalized Linear Models with Negative Binomial Distribution family.

### GAM models

### Regression Tree 
In this section, we look at Regression tree, a simple method that doesn't require any assumption on the distribution of the variables, and moreover, gives a way to select variables.
```{r message = FALSE}
library(rpart)
library(tree)
library(rpart.plot)
library(vip) # for feature importance
```
We first look at Iquitos:

```{r}
iq_mae <- c()
for (iq_plan in iq_plans){
  iq_train <- merged_iq_train %>% slice(iq_plan[1]:iq_plan[2])
  iq_test <- merged_iq_train %>% slice(iq_plan[3]:iq_plan[4])
  tree_iq <- rpart(formula = total_cases ~ ., 
                 data = iq_train,
                 method = "anova")
  rpart.plot(tree_iq)
  ychap.tree_iq <- predict(tree_iq, newdata = iq_train)
  plot(iq_train$total_cases, type='l')
  lines(ychap.tree_iq,col='red')
  ychap.tree_iq <- predict(tree_iq, newdata = iq_test)
  mae(iq_test$total_cases, ychap.tree_iq)
  iq_mae <- append(iq_mae, mae(iq_test$total_cases, ychap.tree_iq))
  plot(iq_test$total_cases, type='l')
  lines(ychap.tree_iq,col='red')
  print(vip(tree_iq, num_features = 10, bar = FALSE)) # return the importance of the features
}

```
```{r}
mean(iq_mae)
```




### Random Forest

### Times Series (ARIMA/SARIMA)

### Gradient Boosting

## Conclusion


